last year i read a blog post from brendan o'connor entitled statistics vs machine learning fight that discussed some of the differences between the two fields andrew gelman responded favorably to this simon blomberg from r's fortunes package to paraphrase provocatively 'machine learning is statistics minus any checking of models and assumptions' brian d ripley about the difference between machine learning and statistics user vienna may season's greetings andrew gelman in that case maybe we should get rid of checking of models and assumptions more often then maybe we'd be able to solve some of the problems that the machine learning people can solve but we can't there was also the statistical modeling the two cultures paper by leo breiman in which argued that statisticians rely too heavily on data modeling and that machine learning techniques are making progress by instead relying on the predictive accuracy of models has the statistics field changed over the last decade in response to these critiques do the two cultures still exist or has statistics grown to embrace machine learning techniques such as neural networks and support vector machines 
what are some of the ways to forecast demographic census with some validation and calibration techniques some of the concerns census blocks vary in sizes as rural areas are a lot larger than condensed urban areas is there a need to account for the area size difference if let's say i have census data dating back to census periods how far can i forecast it into the future if some of the census zone change lightly in boundaries how can i account for that change what are the methods to validate census forecasts for example if i have data for existing census periods should i model the first and test it on the latter two or is there another way what's the state of practice in forecasting census data and what are some of the state of the art methods 
how would you describe in plain english the characteristics that distinguish bayesian from frequentist reasoning 
after taking a statistics course and then trying to help fellow students i noticed one subject that inspires much head desk banging is interpreting the results of statistical hypothesis tests it seems that students easily learn how to perform the calculations required by a given test but get hung up on interpreting the results many computerized tools report test results in terms of p values or t values how would you explain the following points to college students taking their first course in statistics what does a p value mean in relation to the hypothesis being tested are there cases when one should be looking for a high p value or a low p value what is the relationship between a p value and a t value 
there is an old saying correlation does not mean causation when i teach i tend to use the following standard examples to illustrate this point number of storks and birth rate in denmark number of priests in america and alcoholism in the start of the th century it was noted that there was a strong correlation between 'number of radios' and 'number of people in insane asylums' and my favorite pirates cause global warming however i do not have any references for these examples and whilst amusing they are obviously false does anyone have any other good examples 
